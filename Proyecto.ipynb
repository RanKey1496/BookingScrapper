{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/nomiente.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto de aula - Reconocimiento de Patrones\n",
    "## Jhon Jaime Gil Sepulveda - Nelson Javier Posada Flórez\n",
    "\n",
    "Para este proyecto se requiere una base de datos de 100 comentarios de usuarios en el ambito de hotelería. Estos datos deben ser balanceados por lo que se deberán sacar 50 comentarios positivos y 50 comentarios negativos.\n",
    "\n",
    "### Art Hotel Boutique Medellín\n",
    "Hemos elegido este hotel por la cantidad de comentarios disponibles en la página de Booking.com\n",
    "Puedes encontrar el link de este aquí:\n",
    "\n",
    "[Art Hotel Boutique in Booking.com](https://www.booking.com/hotel/co/art.es.html?label=gen173nr-1FCAEoggJCAlhYSDNYBGgyiAEBmAEKwgEKd2luZG93cyAxMMgBDNgBAegBAfgBC5ICAXmoAgM;sid&#tab-main)\n",
    "\n",
    "### Reporte de artículos\n",
    "El reporte se puede encontrar dentro de la carpeta docs/reporte.pdf\n",
    "\n",
    "### Proyecto\n",
    "\n",
    "#### 1. Extracción de comentarios\n",
    "Dado que la recolección de comentarios parecía una tarea sencilla, quisimos automatizar este proceso por lo que se utilizó una técnica para extraer los reviews del hotel llamada [WebScrapping](https://en.wikipedia.org/wiki/Web_scraping)\n",
    "Haciendo uso de la librería Selenium y BeautifulSoup, se realizó el proceso automatico de extracción de los documentos y el código puede ser encontrado dentro de la carpeta \"utils/BookingScrapper.py\"\n",
    "Posteriormente a esto, los comentarios se corrigieron manualmente, estos quedaron guardados en el archivo reviews.txt.\n",
    "\n",
    "#### 2. Caracterización de texto\n",
    "Se hizo el uso de la API [Corpus](http://www.corpus.unam.mx/servicio-freeling/) para obtener el etiquetado morfosintáctico de los comentarios y posteriormente a esto, se utilizó el recurso léxico MLSenticon para obtener los pesos que representaban los datos (Se tuvo en cuenta la polaridad del texto, por lo que a esto se le diseñó un algorimo sencillo también).\n",
    "\n",
    "#### 3. Modelos clasificadores\n",
    "Se utilizó la librería Scikit Learn para obtener las medidas de desempeño de los datos recibidos del anterior punto, se utilizó la métodología de validación Boostraping y se presentaron los datos en una tabla utilizando la libería Pandas.\n",
    "\n",
    "#### 4. TF-IDF y CountVectorizer\n",
    "Nuevamente, haciendo uso de la librería Scikit Learn utilizamos los recursos de TF-IDK y CountVectorizer. Posteriormente a esto, se realizó nuevamente el punto 3 con los valores nuevos y se presentaron los datos en una tabla utilizando la libería Pandas.\n",
    "\n",
    "##### El proyecto también puede ser ejecutado ejecutando \"python Proyecto.py\"\n",
    "\n",
    "[Proyecto en Github](https://github.com/RanKey1496/BookingScrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Se leen los comentarios y se devuelven dos listas con los valores\n",
    "de X y Y\n",
    "\"\"\"\n",
    "def get_database():\n",
    "    db = open('reviews.txt', 'r')\n",
    "    db_data = []\n",
    "    db_target = []\n",
    "    for line in db:\n",
    "        db_data.append(line.split('\\t')[0])\n",
    "        db_target.append(line.split('\\t')[1][:-1])\n",
    "    db.close()\n",
    "    return db_data, db_target\n",
    "\n",
    "\"\"\"\n",
    "Retorna si una palabra existe dentro una lista\n",
    "\"\"\"\n",
    "def exists(array, word):\n",
    "    for i in array:\n",
    "        if (i[0] == word):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\"\"\"\n",
    "´Mira si una palabra es una negación o no\n",
    "\"\"\"\n",
    "def is_negative(text):\n",
    "    low = text.lower()\n",
    "    negatives = ['no', 'pero', 'aunque', 'but', 'ni']\n",
    "    if low in negatives:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\"\"\"\n",
    "Copia cada comentario en un archivo dentro de la carpeta \"reviews\"\n",
    "\"\"\"\n",
    "def write_comments_in_separeted_files():\n",
    "    file = open('reviews.txt', 'r').readlines()\n",
    "    for i, line in enumerate(file):\n",
    "        newFile = open('reviews/review'+str(i)+'.txt', 'w')\n",
    "        newFile.write(file[i].split('\\t')[0])\n",
    "        newFile.close()\n",
    "        \n",
    "\"\"\"\n",
    "Retorna todos los nombres de archivos en la carpeta \"reviews\"\n",
    "\"\"\"        \n",
    "def get_file_names():\n",
    "    for root, dirs, files in os.walk('./reviews'):\n",
    "        return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtiene la lista de valores de la BD de Senticon\n",
    "\"\"\"\n",
    "def get_senticon():\n",
    "    listSenticon = []\n",
    "    mlSenticon = open('MLSenticon.txt', 'r')\n",
    "    for i in mlSenticon:\n",
    "        word = i.split('\\t')\n",
    "        listSenticon.append([word[0], word[1].replace('\\n', '')])\n",
    "    mlSenticon.close()\n",
    "    return listSenticon\n",
    "\n",
    "\"\"\"\n",
    "Hace un POST request a la API Corpues con un archivo que\n",
    "contiene un comentario y esta nos devuelve el objeto JSON\n",
    "de la respuesta\n",
    "\"\"\"\n",
    "def obtain_freeling(fileName):\n",
    "    files = {'file': open(fileName, 'r')}\n",
    "    params = {'outf': 'tagged', 'format': 'json'}\n",
    "    url = 'http://www.corpus.unam.mx/servicio-freeling/analyze.php'\n",
    "    req = requests.post(url, files=files, params=params)\n",
    "    return req.json()\n",
    "\n",
    "\"\"\"\n",
    "Recibe un objeto respuesta de la API Corpus y la BD de Senticon\n",
    "Si la palabra es una negación invierte los pesos del Senticon,\n",
    "posteriormente si la palabra es un adjetivo, si el peso de la palabra\n",
    "por la polaridad es mayor a 0, sumará el valor del peso a la\n",
    "variable de los positivos, en caso contrario a los negativos\n",
    "Si la palabra es un punto y la polaridad es negativa, se invierte la\n",
    "polaridad nuevamente y al finalizar de analizar cada objeto obtenido\n",
    "de la API Corpus retornamos los valores\n",
    "\"\"\"\n",
    "def classification(res, senticon):\n",
    "    x1, x2 = 0, 0\n",
    "    polarity = 1\n",
    "    for r in res:\n",
    "        for word in r:\n",
    "            if (is_negative(word['token'])):\n",
    "                polarity = -1\n",
    "            if (word['tag'][0] == 'A'):\n",
    "                e = exists(senticon, word['lemma'])\n",
    "                if (e):\n",
    "                    if (float(e[1])*polarity > 0):\n",
    "                        x1 += float(e[1])\n",
    "                    else:\n",
    "                        x2 += float(e[1])\n",
    "            if (polarity == -1 and word['token'] == '.'):\n",
    "                polarity = 1\n",
    "    return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retorna la sensibilidad y la especificidad de los datos\n",
    "obtenidos contra los datos de prueba\n",
    "\"\"\"\n",
    "def error_measures(Ypredict, Yreal):\n",
    "    CM = confusion_matrix(Yreal, Ypredict)\n",
    "    \n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    \n",
    "    sens = TP/(TP+FN)\n",
    "    spec = TN/(TN+FP)\n",
    "    return sens, spec\n",
    "\n",
    "\"\"\"\n",
    "Imprime los datos en una tabla haciendo uso de la libreria pandas\n",
    "\"\"\"\n",
    "def table_result(data, index, model):\n",
    "    print('\\n', model)\n",
    "    df = pd.DataFrame(data, index=index)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos clasificadores\n",
    "Todas las funciones retornan los valores de exactitud, sensibilidad y especificidad.\n",
    "Para los modelos clasificadores de Logistic Regression y K-Nearest Neighbors se utilizó la función del modelo .score para obtener la exactitud de los datos, mientras que para el modelo Random Forest se hizo uso del modulo acurracy_score de Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Haciendo uso del modelo Logistic Regression\n",
    "\"\"\"\n",
    "def lr_classification(data, y):\n",
    "    model = LR()\n",
    "    acc = []\n",
    "    sens = []\n",
    "    spec = []\n",
    "    \n",
    "    for i in range(100):\n",
    "        Xtrain, Xtest, Ytrain, Ytest = train_test_split(data, y)\n",
    "        model.fit(Xtrain, Ytrain)\n",
    "        y_pred = model.predict(Xtest)\n",
    "        sen, spc = error_measures(y_pred, Ytest)\n",
    "        sens.append(sen)\n",
    "        spec.append(spc)\n",
    "        acc.append(model.score(Xtest, Ytest))\n",
    "    return acc, sens, spec\n",
    "\n",
    "\"\"\"\n",
    "Haciendo uso del modelo K-Nearest Neighbors\n",
    "\"\"\"\n",
    "def knn_classification(data, y, k):\n",
    "    model = KNN(n_neighbors=k)\n",
    "    acc = []\n",
    "    sens = []\n",
    "    spec = []\n",
    "    \n",
    "    for i in range(100):\n",
    "        Xtrain, Xtest, Ytrain, Ytest = train_test_split(data, y)\n",
    "        model.fit(Xtrain, Ytrain)\n",
    "        y_pred = model.predict(Xtest)\n",
    "        sen, spc = error_measures(y_pred, Ytest)\n",
    "        sens.append(sen)\n",
    "        spec.append(spc)\n",
    "        acc.append(model.score(Xtest, Ytest))\n",
    "    return acc, sens, spec\n",
    "\n",
    "\"\"\"\n",
    "Haciendo uso del modelo Random Forest\n",
    "\"\"\"\n",
    "def rf_classification(data, y, estimators):\n",
    "    model = RF(n_estimators=estimators)\n",
    "    acc = []\n",
    "    sens = []\n",
    "    spec = []\n",
    "    \n",
    "    for i in range(100):\n",
    "        Xtrain, Xtest, Ytrain, Ytest = train_test_split(data, y)\n",
    "        model.fit(Xtrain, Ytrain)\n",
    "        y_pred = model.predict(Xtest)\n",
    "        sen, spc = error_measures(y_pred, Ytest)\n",
    "        sens.append(sen)\n",
    "        spec.append(spc)\n",
    "        acc.append(accuracy_score(Ytest, y_pred))\n",
    "    return acc, sens, spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento\n",
    "Tomando los datos obtenidos de los comentarios, se realiza el procesamiento de los datos de acuerdo a los párametros dados por el profesor y luego imprimiendolos en una tabla usando la librería pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_results(data, target):\n",
    "    lr = lr_classification(data, target)\n",
    "    data = {'Accurancy': '{} +/- {}'.format(round(np.mean(lr[0]),4), round(np.std(lr[0]),2)),\n",
    "            'Sensibility': '{} +/- {}'.format(round(np.mean(lr[1]),4), round(np.std(lr[1]),2)),\n",
    "            'Specificity': '{} +/- {}'.format(round(np.mean(lr[1]),4), round(np.std(lr[2]),2))}\n",
    "    table_result(data, [1], 'Logistic Regression Classification')\n",
    "\n",
    "def knn_results(data, target):\n",
    "    ks = [1,3,5,7,9,15,25]\n",
    "    acc = []\n",
    "    sens = []\n",
    "    spec = []\n",
    "    for i in ks:\n",
    "        knn = knn_classification(data, target, i)\n",
    "        acc.append('{} +/- {}'.format(round(np.mean(knn[0]),4), round(np.std(knn[0]),2)))\n",
    "        sens.append('{} +/- {}'.format(round(np.mean(knn[1]),4), round(np.std(knn[1]),2)))\n",
    "        spec.append('{} +/- {}'.format(round(np.mean(knn[2]),4), round(np.std(knn[2]),2)))\n",
    "    data = {'Accurancy': acc, 'Sensibility': sens, 'Specificity': spec}\n",
    "    df = pd.DataFrame(data, index=ks)\n",
    "    table_result(data, ks, 'K-Nearest Neighbors Classification')\n",
    "    \n",
    "def rf_results(data, target):\n",
    "    n_estimators = [10,20,30,40,50]\n",
    "    acc = []\n",
    "    sens = []\n",
    "    spec = []\n",
    "    for i in n_estimators:\n",
    "        rf = rf_classification(data, target, i)       \n",
    "        acc.append('{} +/- {}'.format(round(np.mean(rf[0]),4), round(np.std(rf[0]),2)))\n",
    "        sens.append('{} +/- {}'.format(round(np.mean(rf[1]),4), round(np.std(rf[1]),2)))\n",
    "        spec.append('{} +/- {}'.format(round(np.mean(rf[2]),4), round(np.std(rf[2]),2)))\n",
    "    data = {'Accurancy': acc, 'Sensibility': sens, 'Specificity': spec}\n",
    "    table_result(data, n_estimators, 'Random Forest Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecución\n",
    "#### En este paso pasará la mágia, así que pongase cómodo y unas gafas por no sabemos que sucederá.\n",
    "\n",
    "\n",
    "<img src=\"img/giphy.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obtener los datos de las bases de datos, tanto comentarios como el MLSenticon.\n",
    "Escribimos los comentarios en archivos separados para facilitar el trabajo con\n",
    "la API Corpus y todos los nombres que hay dentro de la carpeta con los archivos\n",
    "\"\"\"\n",
    "data, target = get_database()\n",
    "senticon = get_senticon()\n",
    "write_comments_in_separeted_files()\n",
    "files = get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Por cada comentario (archivo), haremos la petición a la API Corpus, luego\n",
    "se hará la clasificación con respecto al MLSenticon y sus pesos\n",
    "\"\"\"\n",
    "x = []\n",
    "for i in files:\n",
    "    review = './reviews/' + i\n",
    "    req = obtain_freeling(review)\n",
    "    result = classification(req, senticon)\n",
    "    x.append([result[0], result[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0.75, 0], [0, 0], [0, 0], [3.364, 0], [0.292, 0], [0, 0], [0, 0], [2.416, 0], [0, 0.25], [0.833, 0], [0, 0.612], [2.136, 0], [0.75, 0], [0.5840000000000001, 0], [3.575, 0], [0.833, 0], [1.5, 0], [0, 0.347], [1.2189999999999999, 0], [0, 0], [0, 0], [0, 0], [1.375, 0], [1.3479999999999999, 0], [0.406, 1.666], [2.724, 0], [0.75, -0.575], [0.833, 0], [0, 0], [2.271, 0], [-1.908, 0], [0, 0], [0, 0], [0.675, -0.575], [1.125, 0], [-0.538, 0], [0.75, 0], [0, 0], [1.458, 2.416], [0, 0.16700000000000004], [0.792, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0.17500000000000004, -0.5], [1.174, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1.875, 0], [0, 0], [0, 0], [0, 0], [0.833, 0], [0, 0], [0, 0], [0, 0], [0.625, 0], [0, -0.575], [0, 0], [0, 0], [0, 0], [2.661, 0], [0.25, 0], [0, 0], [0, 0], [2.354, 0], [0, 0], [0, 0], [0, 1.4789999999999999], [0, 0], [0, 0], [0, -0.575], [2.354, 0], [0, -0.25], [0.458, 0], [0, -0.281], [0.75, 0], [0, 0], [0.333, 2.4989999999999997], [0, 0], [1.078, 0], [0.292, -0.833], [0, -0.575], [0.292, 0], [0, 0], [0.792, 0], [0, 0], [0.833, 0], [0, -1.5], [0, 0], [0, -0.281], [0, 0], [0, 0], [0, 0], [0, 0], [0.292, 0], [2.226, 0], [0, 0], [0, 0], [0, 0.313], [0.7969999999999999, 0], [0, 0], [0.625, 0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Los valores obtenidos por la API Corpus y MLSenticon\n",
    "\"\"\"\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression Classification\n",
      "         Accurancy      Sensibility      Specificity\n",
      "1  0.4296 +/- 0.07  0.4681 +/- 0.33  0.4681 +/- 0.35\n",
      "\n",
      " K-Nearest Neighbors Classification\n",
      "          Accurancy      Sensibility      Specificity\n",
      "1   0.4682 +/- 0.09   0.421 +/- 0.22    0.517 +/- 0.3\n",
      "3   0.4857 +/- 0.09  0.4634 +/- 0.24   0.5197 +/- 0.3\n",
      "5   0.5043 +/- 0.09  0.4447 +/- 0.25  0.5848 +/- 0.31\n",
      "7   0.5043 +/- 0.08  0.4452 +/- 0.23  0.5787 +/- 0.31\n",
      "9    0.4907 +/- 0.1  0.4286 +/- 0.25   0.5758 +/- 0.3\n",
      "15  0.5139 +/- 0.09  0.4492 +/- 0.21  0.6051 +/- 0.29\n",
      "25   0.4889 +/- 0.1  0.4503 +/- 0.24    0.57 +/- 0.29\n",
      "\n",
      " Random Forest Classification\n",
      "          Accurancy      Sensibility      Specificity\n",
      "10  0.4707 +/- 0.08   0.292 +/- 0.17   0.6698 +/- 0.2\n",
      "20  0.4729 +/- 0.09  0.2845 +/- 0.16  0.6747 +/- 0.21\n",
      "30  0.4811 +/- 0.08  0.2864 +/- 0.13   0.6998 +/- 0.2\n",
      "40  0.4946 +/- 0.08  0.2687 +/- 0.13  0.7282 +/- 0.16\n",
      "50  0.4804 +/- 0.08  0.2855 +/- 0.14  0.6917 +/- 0.21\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluamos estos datos con el modelo Logistic Regression\n",
    "\"\"\"\n",
    "lr_results(x, target)\n",
    "\n",
    "\"\"\"\n",
    "Evaluamos estos datos con el modelo K-Nearest Neighbors\n",
    "\"\"\"\n",
    "knn_results(x, target)\n",
    "\n",
    "\"\"\"\n",
    "Evaluamos estos datos con el modelo Random Forest\n",
    "\"\"\"\n",
    "rf_results(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression Classification\n",
      "         Accurancy     Sensibility      Specificity\n",
      "1  0.8946 +/- 0.06  0.8636 +/- 0.1  0.8636 +/- 0.08\n",
      "\n",
      " K-Nearest Neighbors Classification\n",
      "          Accurancy      Sensibility      Specificity\n",
      "1   0.8639 +/- 0.06  0.9144 +/- 0.07  0.8144 +/- 0.09\n",
      "3   0.8879 +/- 0.05  0.9258 +/- 0.06  0.8503 +/- 0.08\n",
      "5   0.8861 +/- 0.05  0.9244 +/- 0.07  0.8551 +/- 0.08\n",
      "7   0.8825 +/- 0.06  0.9133 +/- 0.08  0.8565 +/- 0.08\n",
      "9    0.875 +/- 0.05  0.8892 +/- 0.08  0.8681 +/- 0.08\n",
      "15  0.8668 +/- 0.05  0.8873 +/- 0.08  0.8602 +/- 0.09\n",
      "25  0.8564 +/- 0.06  0.8553 +/- 0.08  0.8683 +/- 0.09\n",
      "\n",
      " Random Forest Classification\n",
      "          Accurancy      Sensibility      Specificity\n",
      "10  0.7746 +/- 0.08  0.8107 +/- 0.12  0.7517 +/- 0.14\n",
      "20  0.7882 +/- 0.08  0.8429 +/- 0.11  0.7497 +/- 0.14\n",
      "30  0.8011 +/- 0.07    0.861 +/- 0.1  0.7531 +/- 0.12\n",
      "40  0.7943 +/- 0.07  0.8818 +/- 0.09  0.7195 +/- 0.12\n",
      "50  0.7975 +/- 0.07   0.8627 +/- 0.1   0.746 +/- 0.12\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Obtenemos los datos nuevamente y con la representación de los textos\n",
    "haciendo uso de TF-IDF Vectorizer de Scikit Learn realizamos una \n",
    "nueva evaluación con los 3 modelos de clasificación\n",
    "\"\"\"\n",
    "data, target = get_database()\n",
    "vectorTF = TfidfVectorizer()\n",
    "vectorTF.fit(data)\n",
    "bow = vectorTF.transform(data)\n",
    "lr_results(bow, target)\n",
    "knn_results(bow, target)\n",
    "rf_results(bow, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Logistic Regression Classification\n",
      "         Accurancy      Sensibility      Specificity\n",
      "1  0.8639 +/- 0.07  0.9168 +/- 0.07  0.9168 +/- 0.11\n",
      "\n",
      " K-Nearest Neighbors Classification\n",
      "          Accurancy      Sensibility      Specificity\n",
      "1   0.6393 +/- 0.08  0.8487 +/- 0.19  0.4231 +/- 0.19\n",
      "3   0.5786 +/- 0.09  0.9784 +/- 0.04  0.1869 +/- 0.13\n",
      "5   0.5593 +/- 0.09  0.9992 +/- 0.01  0.1213 +/- 0.09\n",
      "7   0.5518 +/- 0.09  0.9994 +/- 0.01  0.0914 +/- 0.08\n",
      "9   0.5361 +/- 0.09      1.0 +/- 0.0  0.0659 +/- 0.05\n",
      "15  0.5229 +/- 0.08      1.0 +/- 0.0   0.058 +/- 0.05\n",
      "25  0.5193 +/- 0.09      1.0 +/- 0.0    0.04 +/- 0.05\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ahora nuevamente, obtenemos los datos y con la matriz de terminos de\n",
    "documentos CountVectorizer de Scikit Learn realizamos una nueva evaluación\n",
    "con los 3 modelos de clasificación\n",
    "\"\"\"\n",
    "data, target = get_database()\n",
    "vectorCount = CountVectorizer(ngram_range=(1,2))\n",
    "vectorCount.fit(data)\n",
    "bow = vectorCount.transform(data)\n",
    "lr_results(bow, target)\n",
    "knn_results(bow, target)\n",
    "rf_results(bow, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esto es todo señores\n",
    "\n",
    "<img src=\"img/miente.png\">\n",
    "\n",
    "##### PD: Profe no nos rebaje por esta estupidez :'v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
